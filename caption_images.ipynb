{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (100, 50, 50, 1)\n",
      "y_train: (100, 16)\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "h5f = h5py.File('dataset/train_data.h5', 'r')\n",
    "x_train = h5f['x_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "h5f.close()\n",
    "\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_val: (25, 50, 50, 1)\n",
      "y_val: (25, 16)\n"
     ]
    }
   ],
   "source": [
    "# validation data\n",
    "h5f = h5py.File('dataset/val_data.h5', 'r')\n",
    "x_val = h5f['x_val'][:]\n",
    "y_val = h5f['y_val'][:]\n",
    "h5f.close()\n",
    "\n",
    "print('x_val:', x_val.shape)\n",
    "print('y_val:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vocabulary and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    print('Creating word to vec map...')\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float32)\n",
    "    print('Done!')\n",
    "    return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word to vec map...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# load embeddings\n",
    "word_to_vec_map = read_glove_vecs('{}/glove.6B.100d.txt'.format('dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign embeddings values to tokens\n",
    "size = word_to_vec_map['unk'].shape\n",
    "\n",
    "word_to_vec_map['<sos>'] = np.random.uniform(low=-1.0, high=1.0, size=size)\n",
    "word_to_vec_map['<eos>'] = np.random.uniform(low=-1.0, high=1.0, size=size)\n",
    "word_to_vec_map['<pad>'] = np.random.uniform(low=-1.0, high=1.0, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "with open('dataset/vocabulary.pickle', 'rb') as vocab_file:\n",
    "    vocabulary_dict = pickle.load(vocab_file)\n",
    "\n",
    "vocabulary = vocabulary_dict['vocabulary']\n",
    "word_to_index = vocabulary_dict['word_to_index']\n",
    "index_to_word = vocabulary_dict['index_to_word']\n",
    "\n",
    "# number of words in vocabulary\n",
    "num_words = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input to encoder is the numpy array of the image\n",
    "encoder_input_data = x_train\n",
    "encoder_input_data_val = x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output data for the decoder is identical, except shifted one time-step\n",
    "\n",
    "# training\n",
    "decoder_input_data = y_train[:, :-1]\n",
    "decoder_output_data = y_train[:, 1:]\n",
    "\n",
    "# validation\n",
    "decoder_input_data_val = y_val[:, :-1]\n",
    "decoder_output_data_val = y_val[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Image Model (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.layers import Conv2D, Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from capsule_layers import CapsuleLayer, PrimaryCap, Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_encoder(encoder_input, num_caption_caps, routings):\n",
    "    \"\"\" Create the encoder model\n",
    "        @params:\n",
    "        :encoder_input -- input tensor for the image to be given to the capsnet model\n",
    "        :num_caption_caps -- number of capsules in caption caps layer\n",
    "        :routings -- number of routings in the dynamic routing algorithm\n",
    "        \n",
    "        @return:\n",
    "        :encoder_output -- output of the capsnet model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Layers 1-3: Three conventional Conv2D layers\n",
    "    conv1 = Conv2D(filters=96, kernel_size=13, strides=4, padding='valid', activation='relu', name='conv1')(encoder_input)\n",
    "    conv2 = Conv2D(filters=96, kernel_size=5, strides=2, padding='valid', activation='relu', name='conv2')(conv1)\n",
    "    conv3 = Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv3')(conv2)\n",
    "    \n",
    "    # Layer 4: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
    "    primary_caps = PrimaryCap(conv3, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 5: Capsule layer. Routing algorithm works here.\n",
    "    caption_caps = CapsuleLayer(num_capsule=num_caption_caps, dim_capsule=16, routings=routings, name='caption_caps')(primary_caps)\n",
    "\n",
    "    encoder_output = Length(name='capsnet')(caption_caps)\n",
    "    \n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_shape = x_train.shape[1:]  # (img_size, img_size, channels)\n",
    "num_caption_caps = 20  # Number of capsules in caption caps layer\n",
    "routings = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for input image\n",
    "encoder_input = Input(shape=encoder_input_shape, name='encoder_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of capsnet\n",
    "encoder_output = connect_encoder(encoder_input, num_caption_caps, routings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define caption model (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(word_to_index, word_to_vec_map, num_words):\n",
    "    \"\"\" Create a Keras Embedding() layer and load in pre-trained GloVe 100-dimensional vectors\n",
    "        @params:\n",
    "        :word_to_index -- dictionary containing the each word mapped to its index\n",
    "        :word_to_vec_map -- dictionary mapping words to their GloVe vector representation\n",
    "        :num_words -- number of words in the vocabulary\n",
    "        \n",
    "        @return:\n",
    "        :decoder_embedding -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary_length = num_words + 1  # adding 1 to fit Keras embedding (requirement)\n",
    "    embedding_dimensions = word_to_vec_map['unk'].shape[0]  # define dimensionality of GloVe word vectors (= 100)\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocabulary_length, embedding_dimensions))  # initialize with zeros\n",
    "    for word, index in word_to_index.items():\n",
    "        try:\n",
    "            embedding_matrix[index, :] = word_to_vec_map[word]\n",
    "        except KeyError:\n",
    "            embedding_matrix[index, :] = word_to_vec_map['unk']\n",
    "    \n",
    "    # we don't want the embeddings to be updated, thus trainable parameter is set to False\n",
    "    decoder_embedding = Embedding(vocabulary_length, embedding_dimensions, trainable=False)\n",
    "    decoder_embedding.build((None,))\n",
    "    decoder_embedding.set_weights([embedding_matrix])  # with this the layer is now pretrained\n",
    "    \n",
    "    return decoder_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal state size of LSTM layers in the RNN\n",
    "state_size = 512\n",
    "\n",
    "# initial states for the LSTM cells\n",
    "decoder_initial_hidden_state = Input(shape=(state_size,), name='decoder_initial_hidden_state')  # encoder ouput\n",
    "decoder_initial_cell_state = Input(shape=(state_size,), name='decoder_initial_cell_state')\n",
    "\n",
    "# The ouput of the encoder is given to the LSTM as an initial state,\n",
    "# so the dimensions of the encoder_output and decoder_initial_hidden_state must match.\n",
    "# Thus, a fully-connected layer is used to map the vectors from num_caption_caps to state_size elements.\n",
    "decoder_transfer_map = Dense(state_size, activation='tanh', name='decoder_transfer_map')\n",
    "\n",
    "# input for the token sequences to the decoder\n",
    "# using 'None' in the shape means that the token-sequences can have arbitrary lengths\n",
    "decoder_input = Input(shape=(None,), name='decoder_input')\n",
    "\n",
    "# pretrained embedding layer\n",
    "decoder_embedding = create_embedding_layer(word_to_index, word_to_vec_map, num_words)\n",
    "\n",
    "# LSTM layers of the decoder\n",
    "lstm1 = LSTM(state_size, return_sequences=True, name='lstm1')\n",
    "lstm2 = LSTM(state_size, return_sequences=True, name='lstm2')\n",
    "lstm3 = LSTM(state_size, return_sequences=True, name='lstm3')\n",
    "\n",
    "# output of the decoder model\n",
    "# the activation-function is set to 'linear' because there is a bug in Keras\n",
    "# so a custom loss-function has to be made, which is done below\n",
    "decoder_dense = Dense(num_words, activation='linear', name='decoder_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_decoder(hidden_state, cell_state, model_type='train'):\n",
    "    \"\"\" Create the decoder model\n",
    "        @params:\n",
    "        :hidden_state -- hidden state of the lstm, can be the output of the encoder\n",
    "        :cell_state -- cell state of the lstm\n",
    "        :model_type -- specifies whether to apply the transfer map on the hidden state\n",
    "        \n",
    "        @return:\n",
    "        :decoder_output -- output of the RNN model\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_type == 'train':\n",
    "        hidden_state = decoder_transfer_map(hidden_state)\n",
    "    \n",
    "    # Layer 1: embedding layer\n",
    "    decoder_network = decoder_embedding(decoder_input)\n",
    "    \n",
    "    # Layers 2-4\n",
    "    decoder_network = lstm1(decoder_network, initial_state=[hidden_state, cell_state])\n",
    "    decoder_network = lstm2(decoder_network, initial_state=[hidden_state, cell_state])\n",
    "    decoder_network = lstm3(decoder_network, initial_state=[hidden_state, cell_state])\n",
    "    \n",
    "    # Connect the final dense layer that converts to one-hot encoded arrays\n",
    "    decoder_output = decoder_dense(decoder_network)\n",
    "    \n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "decoder_output = connect_decoder(\n",
    "    hidden_state=encoder_output,\n",
    "    cell_state=decoder_initial_cell_state\n",
    ")\n",
    "\n",
    "model_train = Model(\n",
    "    inputs=[encoder_input, decoder_input, decoder_initial_cell_state],\n",
    "    outputs=[decoder_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "model_encoder = Model(inputs=[encoder_input], outputs=[encoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3\n",
    "decoder_output = connect_decoder(\n",
    "    hidden_state=decoder_initial_hidden_state,\n",
    "    cell_state=decoder_initial_cell_state,\n",
    "    model_type='decoder'\n",
    ")\n",
    "\n",
    "model_decoder = Model(\n",
    "    inputs=[decoder_input, decoder_initial_hidden_state, decoder_initial_cell_state],\n",
    "    outputs=[decoder_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 50, 50, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 20, 20, 256)  31232       encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)      (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 1152, 8)      0           primarycap_conv2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 1152, 8)      0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "caption_caps (CapsuleLayer)     (None, 20, 16)       2949120     primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (Length)                (None, 20)           0           caption_caps[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    957300      decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_transfer_map (Dense)    (None, 512)          10752       capsnet[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_initial_cell_state (Inp (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, None, 512)    1255424     embedding_1[0][0]                \n",
      "                                                                 decoder_transfer_map[0][0]       \n",
      "                                                                 decoder_initial_cell_state[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, None, 512)    2099200     lstm1[0][0]                      \n",
      "                                                                 decoder_transfer_map[0][0]       \n",
      "                                                                 decoder_initial_cell_state[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lstm3 (LSTM)                    (None, None, 512)    2099200     lstm2[0][0]                      \n",
      "                                                                 decoder_transfer_map[0][0]       \n",
      "                                                                 decoder_initial_cell_state[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 9572)   4910436     lstm3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 19,621,336\n",
      "Trainable params: 18,664,036\n",
      "Non-trainable params: 957,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss between y_true and y_pred.\n",
    "    \n",
    "    y_true is a 2-rank tensor with the desired output.\n",
    "    The shape is [batch_size, sequence_length] and it\n",
    "    contains sequences of integer-tokens.\n",
    "\n",
    "    y_pred is the decoder's output which is a 3-rank tensor\n",
    "    with shape [batch_size, sequence_length, num_words]\n",
    "    so that for each sequence in the batch there is a one-hot\n",
    "    encoded array of length num_words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the loss. This outputs a\n",
    "    # 2-rank tensor of shape [batch_size, sequence_length]\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "\n",
    "    # Keras may reduce this across the first axis (the batch)\n",
    "    # but the semantics are unclear, so to be sure we use\n",
    "    # the loss across the entire 2-rank tensor, we reduce it\n",
    "    # to a single scalar with the mean function.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)\n",
    "\n",
    "# There is a bug in Keras due to which it cannot automatically deduce the correct shape of decoder's output data.\n",
    "# We therefore need to manually create a placeholder variable for the decoder's output.\n",
    "# The shape is set to (None, None) which means the batch can have an arbitrary number of sequences,\n",
    "# which can have an arbitrary number of integer-tokens.\n",
    "decoder_target = K.placeholder(dtype='int64', shape=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=sparse_cross_entropy,\n",
    "    target_tensors=[decoder_target]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback Functions\n",
    "\n",
    "During training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for writing checkpoints during training\n",
    "path_checkpoint = 'checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(\n",
    "    filepath=path_checkpoint,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for stopping the optimization when performance worsens on the validation-set\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for writing the TensorBoard log during training\n",
    "callback_tensorboard = TensorBoard(log_dir='./logs/', histogram_freq=0, write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_early_stopping, callback_checkpoint, callback_tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error trying to load checkpoint.\n",
      "Unable to open file (unable to open file: name = 'checkpoint.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_train.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = {\n",
    "    'encoder_input': encoder_input_data,\n",
    "    'decoder_input': decoder_input_data,\n",
    "    'decoder_initial_cell_state': np.zeros((x_train.shape[0], state_size))\n",
    "}\n",
    "\n",
    "y_data = {\n",
    "    'decoder_output': decoder_output_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_val = {\n",
    "    'encoder_input': encoder_input_data_val,\n",
    "    'decoder_input': decoder_input_data_val,\n",
    "    'decoder_initial_cell_state': np.zeros((x_val.shape[0], state_size))\n",
    "}\n",
    "\n",
    "y_data_val = {\n",
    "    'decoder_output': decoder_output_data_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 25 samples\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 5.9389 - val_loss: 4.7809\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.78090, saving model to checkpoint.keras\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 4.4068 - val_loss: 4.6314\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.78090 to 4.63143, saving model to checkpoint.keras\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 14s 142ms/step - loss: 4.0880 - val_loss: 4.5248\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.63143 to 4.52482, saving model to checkpoint.keras\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 15s 150ms/step - loss: 3.8598 - val_loss: 4.5455\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.52482\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 3.7483 - val_loss: 4.6342\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.52482\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 3.6290 - val_loss: 4.5924\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.52482\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f815f85df60>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    batch_size=100,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(x_data_val, y_data_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
