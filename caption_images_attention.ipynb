{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (10000, 250, 250, 3)\n",
      "y_train: (10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "h5f = h5py.File('dataset/train_data.h5', 'r')\n",
    "x_train = h5f['x_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "h5f.close()\n",
    "\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_val: (2500, 250, 250, 3)\n",
      "y_val: (2500, 16)\n"
     ]
    }
   ],
   "source": [
    "# validation data\n",
    "h5f = h5py.File('dataset/val_data.h5', 'r')\n",
    "x_val = h5f['x_val'][:]\n",
    "y_val = h5f['y_val'][:]\n",
    "h5f.close()\n",
    "\n",
    "print('x_val:', x_val.shape)\n",
    "print('y_val:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vocabulary and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    print('Creating word to vec map...')\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float32)\n",
    "    print('Done!')\n",
    "    return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word to vec map...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# load embeddings\n",
    "word_to_vec_map = read_glove_vecs('{}/glove.6B.100d.txt'.format('dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign embeddings values to tokens\n",
    "size = word_to_vec_map['unk'].shape\n",
    "\n",
    "word_to_vec_map['<sos>'] = np.random.uniform(low=-1.0, high=1.0, size=size)\n",
    "word_to_vec_map['<eos>'] = np.random.uniform(low=-1.0, high=1.0, size=size)\n",
    "word_to_vec_map['<pad>'] = np.random.uniform(low=-1.0, high=1.0, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "with open('dataset/vocabulary.pickle', 'rb') as vocab_file:\n",
    "    vocabulary_dict = pickle.load(vocab_file)\n",
    "\n",
    "vocabulary = vocabulary_dict['vocabulary']\n",
    "word_to_index = vocabulary_dict['word_to_index']\n",
    "index_to_word = vocabulary_dict['index_to_word']\n",
    "\n",
    "# number of words in vocabulary\n",
    "num_words = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input to encoder is the numpy array of the image\n",
    "encoder_input_data = x_train\n",
    "encoder_input_data_val = x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output data for the decoder is identical, except shifted one time-step\n",
    "\n",
    "# training\n",
    "decoder_input_data = y_train[:, :-1]\n",
    "decoder_output_data = y_train[:, 1:]\n",
    "\n",
    "# validation\n",
    "decoder_input_data_val = y_val[:, :-1]\n",
    "decoder_output_data_val = y_val[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.layers import Conv2D, Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from capsule_layers import CapsuleLayer, PrimaryCap, Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal state size of LSTM layers in the RNN\n",
    "state_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Image Model (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_encoder(encoder_input, num_caption_caps, routings):\n",
    "    \"\"\" Create the encoder model\n",
    "        @params:\n",
    "        :encoder_input -- input tensor for the image to be given to the capsnet model\n",
    "        :num_caption_caps -- number of capsules in caption caps layer\n",
    "        :routings -- number of routings in the dynamic routing algorithm\n",
    "        \n",
    "        @return:\n",
    "        :encoder_output -- output of the capsnet model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Layers 1-3: Three conventional Conv2D layers\n",
    "    conv1 = Conv2D(filters=96, kernel_size=13, strides=4, padding='valid', activation='relu', name='conv1')(encoder_input)\n",
    "    conv2 = Conv2D(filters=96, kernel_size=5, strides=2, padding='valid', activation='relu', name='conv2')(conv1)\n",
    "    conv3 = Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv3')(conv2)\n",
    "    \n",
    "    # Layer 4: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
    "    primary_caps = PrimaryCap(conv3, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 5: Capsule layer. Routing algorithm works here.\n",
    "    caption_caps = CapsuleLayer(num_capsule=num_caption_caps, dim_capsule=16, routings=routings, name='caption_caps')(primary_caps)\n",
    "\n",
    "    encoder_output = Length(name='capsnet')(caption_caps)\n",
    "    \n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_shape = x_train.shape[1:]  # (img_size, img_size, channels)\n",
    "num_caption_caps = 10  # Number of capsules in caption caps layer\n",
    "routings = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for input image\n",
    "encoder_input = Input(shape=encoder_input_shape, name='encoder_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of capsnet\n",
    "encoder_output = connect_encoder(encoder_input, num_caption_caps, routings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ouput of the encoder is given to the LSTM as an initial state,\n",
    "# so the dimensions of the encoder_output and decoder_initial_hidden_state must match.\n",
    "# Thus, a fully-connected layer is used to map the vectors from num_caption_caps to state_size elements.\n",
    "decoder_transfer_map = Dense(state_size, activation='tanh', name='decoder_transfer_map')\n",
    "\n",
    "# fully-connected layer form of the encoder output\n",
    "dense_encoder_output = decoder_transfer_map(encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define caption model (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(word_to_index, word_to_vec_map, num_words):\n",
    "    \"\"\" Create a Keras Embedding() layer and load in pre-trained GloVe 100-dimensional vectors\n",
    "        @params:\n",
    "        :word_to_index -- dictionary containing the each word mapped to its index\n",
    "        :word_to_vec_map -- dictionary mapping words to their GloVe vector representation\n",
    "        :num_words -- number of words in the vocabulary\n",
    "        \n",
    "        @return:\n",
    "        :decoder_embedding -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary_length = num_words + 1  # adding 1 to fit Keras embedding (requirement)\n",
    "    embedding_dimensions = word_to_vec_map['unk'].shape[0]  # define dimensionality of GloVe word vectors (= 100)\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocabulary_length, embedding_dimensions))  # initialize with zeros\n",
    "    for word, index in word_to_index.items():\n",
    "        try:\n",
    "            embedding_matrix[index, :] = word_to_vec_map[word]\n",
    "        except KeyError:\n",
    "            embedding_matrix[index, :] = word_to_vec_map['unk']\n",
    "    \n",
    "    # we don't want the embeddings to be updated, thus trainable parameter is set to False\n",
    "    decoder_embedding = Embedding(vocabulary_length, embedding_dimensions, trainable=False)\n",
    "    decoder_embedding.build((None,))\n",
    "    decoder_embedding.set_weights([embedding_matrix])  # with this the layer is now pretrained\n",
    "    \n",
    "    return decoder_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector, Activation, Concatenate, Dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    \"\"\"Softmax activation function.\n",
    "    # Arguments\n",
    "        x : Tensor.\n",
    "        axis: Integer, axis along which the softmax normalization is applied.\n",
    "    # Returns\n",
    "        Tensor, output of softmax transformation.\n",
    "    # Raises\n",
    "        ValueError: In case `dim(x) == 1`.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = y_train.shape[-1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(seq_len)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_step_attention\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
    "    e = densor(concat)\n",
    "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(e)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_s = 128\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(num_words, activation=softmax)  # should be embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(seq_len,), name='decoder_input')\n",
    "\n",
    "# initial states for the LSTM cells\n",
    "decoder_initial_hidden_state = Input(shape=(state_size,), name='decoder_initial_hidden_state')  # encoder ouput\n",
    "decoder_initial_cell_state = Input(shape=(state_size,), name='decoder_initial_cell_state')\n",
    "\n",
    "# pretrained embedding layer\n",
    "decoder_embedding = create_embedding_layer(word_to_index, word_to_vec_map, num_words)\n",
    "\n",
    "# LSTM layer of the decoder\n",
    "lstm = LSTM(state_size, return_sequences=True, name='lstm')\n",
    "\n",
    "# output of the decoder model\n",
    "# the activation-function is set to 'linear' because there is a bug in Keras\n",
    "# so a custom loss-function has to be made, which is done below\n",
    "# decoder_dense = Dense(num_words, activation='linear', name='decoder_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def decoder_out(decoder_input, hidden_state, cell_state):\n",
    "    \n",
    "    decoder_network = decoder_embedding(decoder_input)\n",
    "    \n",
    "    s = hidden_state\n",
    "    c = cell_state\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = lstm(decoder_network, initial_state=[hidden_state, cell_state])\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(seq_len):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "model_train_out = decoder_out(\n",
    "    decoder_input=decoder_input,\n",
    "    hidden_state=dense_encoder_output,\n",
    "    cell_state=decoder_initial_cell_state\n",
    ")\n",
    "\n",
    "model_train = Model(inputs=[encoder_input, decoder_input, decoder_initial_cell_state], outputs=model_train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "model_encoder = Model(inputs=[encoder_input], outputs=[dense_encoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3\n",
    "model_decoder_out = decoder_out(\n",
    "    decoder_input=decoder_input,\n",
    "    hidden_state=decoder_initial_hidden_state,\n",
    "    cell_state=decoder_initial_cell_state\n",
    ")\n",
    "\n",
    "model_decoder = Model(\n",
    "    inputs=[decoder_input, decoder_initial_hidden_state, decoder_initial_cell_state],\n",
    "    outputs=model_decoder_out\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 250, 250, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 60, 60, 96)   48768       encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 28, 28, 96)   230496      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 20, 20, 256)  1990912     conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)      (None, 6, 6, 256)    5308672     conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 1152, 8)      0           primarycap_conv2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 1152, 8)      0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "caption_caps (CapsuleLayer)     (None, 10, 16)       1474560     primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (Length)                (None, 10)           0           caption_caps[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 15, 100)      957300      decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_transfer_map (Dense)    (None, 128)          1408        capsnet[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_initial_cell_state (Inp (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 15, 128)      117248      embedding_1[0][0]                \n",
      "                                                                 decoder_transfer_map[0][0]       \n",
      "                                                                 decoder_initial_cell_state[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 15, 128)      0           decoder_transfer_map[0][0]       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 15, 256)      0           lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[10][0]           \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[11][0]           \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[12][0]           \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[13][0]           \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 repeat_vector_1[14][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 15, 1)        257         concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "                                                                 concatenate_1[10][0]             \n",
      "                                                                 concatenate_1[11][0]             \n",
      "                                                                 concatenate_1[12][0]             \n",
      "                                                                 concatenate_1[13][0]             \n",
      "                                                                 concatenate_1[14][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 15, 1)        0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "                                                                 dense_1[10][0]                   \n",
      "                                                                 dense_1[11][0]                   \n",
      "                                                                 dense_1[12][0]                   \n",
      "                                                                 dense_1[13][0]                   \n",
      "                                                                 dense_1[14][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[10][0]         \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[11][0]         \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[12][0]         \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[13][0]         \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 attention_weights[14][0]         \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 131584      dot_1[0][0]                      \n",
      "                                                                 decoder_transfer_map[0][0]       \n",
      "                                                                 decoder_initial_cell_state[0][0] \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "                                                                 dot_1[10][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[9][2]                     \n",
      "                                                                 dot_1[11][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[10][2]                    \n",
      "                                                                 dot_1[12][0]                     \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[11][2]                    \n",
      "                                                                 dot_1[13][0]                     \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[12][2]                    \n",
      "                                                                 dot_1[14][0]                     \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[13][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 9572)         1234788     lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[14][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,495,993\n",
      "Trainable params: 10,538,693\n",
      "Non-trainable params: 957,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss between y_true and y_pred.\n",
    "    \n",
    "    y_true is a 2-rank tensor with the desired output.\n",
    "    The shape is [batch_size, sequence_length] and it\n",
    "    contains sequences of integer-tokens.\n",
    "\n",
    "    y_pred is the decoder's output which is a 3-rank tensor\n",
    "    with shape [batch_size, sequence_length, num_words]\n",
    "    so that for each sequence in the batch there is a one-hot\n",
    "    encoded array of length num_words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the loss. This outputs a\n",
    "    # 2-rank tensor of shape [batch_size, sequence_length]\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "\n",
    "    # Keras may reduce this across the first axis (the batch)\n",
    "    # but the semantics are unclear, so to be sure we use\n",
    "    # the loss across the entire 2-rank tensor, we reduce it\n",
    "    # to a single scalar with the mean function.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "# There is a bug in Keras due to which it cannot automatically deduce the correct shape of decoder's output data.\n",
    "# We therefore need to manually create a placeholder variable for the decoder's output.\n",
    "# The shape is set to (None, None) which means the batch can have an arbitrary number of sequences,\n",
    "# which can have an arbitrary number of integer-tokens.\n",
    "# decoder_target = K.placeholder(dtype='int64', shape=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback Functions\n",
    "\n",
    "During training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for writing checkpoints during training\n",
    "path_checkpoint = 'checkpoint_att.keras'\n",
    "callback_checkpoint = ModelCheckpoint(\n",
    "    filepath=path_checkpoint,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for stopping the optimization when performance worsens on the validation-set\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for writing the TensorBoard log during training\n",
    "callback_tensorboard = TensorBoard(log_dir='./logs/', histogram_freq=0, write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_early_stopping, callback_checkpoint, callback_tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_train.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = {\n",
    "    'encoder_input': encoder_input_data,\n",
    "    'decoder_input': decoder_input_data,\n",
    "    'decoder_initial_cell_state': np.zeros((x_train.shape[0], state_size))\n",
    "}\n",
    "\n",
    "y_data = {\n",
    "    'decoder_output': decoder_output_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_val = {\n",
    "    'encoder_input': encoder_input_data_val,\n",
    "    'decoder_input': decoder_input_data_val,\n",
    "    'decoder_initial_cell_state': np.zeros((x_val.shape[0], state_size))\n",
    "}\n",
    "\n",
    "y_data_val = {\n",
    "    'decoder_output': decoder_output_data_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 10000)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data.swapaxes(0,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 373s 37ms/step - loss: 74.8625 - dense_2_loss: 1.8513 - dense_2_acc: 0.3929 - dense_2_acc_1: 1.0000e-04 - dense_2_acc_2: 0.0000e+00 - dense_2_acc_3: 0.0000e+00 - dense_2_acc_4: 0.0000e+00 - dense_2_acc_5: 0.0000e+00 - dense_2_acc_6: 0.0000e+00 - dense_2_acc_7: 0.0000e+00 - dense_2_acc_8: 0.0090 - dense_2_acc_9: 0.1624 - dense_2_acc_10: 0.3686 - dense_2_acc_11: 0.5814 - dense_2_acc_12: 0.7394 - dense_2_acc_13: 0.8408 - dense_2_acc_14: 0.8996\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thegeek_004/.conda/envs/ai/lib/python3.6/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,dense_2_loss,dense_2_acc,dense_2_acc_1,dense_2_acc_2,dense_2_acc_3,dense_2_acc_4,dense_2_acc_5,dense_2_acc_6,dense_2_acc_7,dense_2_acc_8,dense_2_acc_9,dense_2_acc_10,dense_2_acc_11,dense_2_acc_12,dense_2_acc_13,dense_2_acc_14\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
      "/home/thegeek_004/.conda/envs/ai/lib/python3.6/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 320s 32ms/step - loss: 69.3784 - dense_2_loss: 1.3979 - dense_2_acc: 0.7126 - dense_2_acc_1: 0.0014 - dense_2_acc_2: 0.0028 - dense_2_acc_3: 0.0000e+00 - dense_2_acc_4: 0.0000e+00 - dense_2_acc_5: 0.0000e+00 - dense_2_acc_6: 0.0000e+00 - dense_2_acc_7: 0.0000e+00 - dense_2_acc_8: 0.0091 - dense_2_acc_9: 0.1640 - dense_2_acc_10: 0.3722 - dense_2_acc_11: 0.5872 - dense_2_acc_12: 0.7474 - dense_2_acc_13: 0.8498 - dense_2_acc_14: 0.9093\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 312s 31ms/step - loss: 68.2283 - dense_2_loss: 1.2987 - dense_2_acc: 0.7126 - dense_2_acc_1: 0.0014 - dense_2_acc_2: 0.0189 - dense_2_acc_3: 0.0047 - dense_2_acc_4: 5.0000e-04 - dense_2_acc_5: 0.0000e+00 - dense_2_acc_6: 1.0000e-04 - dense_2_acc_7: 0.0000e+00 - dense_2_acc_8: 0.0091 - dense_2_acc_9: 0.1640 - dense_2_acc_10: 0.3722 - dense_2_acc_11: 0.5872 - dense_2_acc_12: 0.7474 - dense_2_acc_13: 0.8498 - dense_2_acc_14: 0.9093\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 313s 31ms/step - loss: 66.8697 - dense_2_loss: 1.1476 - dense_2_acc: 0.7126 - dense_2_acc_1: 0.0014 - dense_2_acc_2: 0.0249 - dense_2_acc_3: 0.0267 - dense_2_acc_4: 1.0000e-04 - dense_2_acc_5: 0.0000e+00 - dense_2_acc_6: 0.0000e+00 - dense_2_acc_7: 0.0000e+00 - dense_2_acc_8: 0.0091 - dense_2_acc_9: 0.1640 - dense_2_acc_10: 0.3722 - dense_2_acc_11: 0.5872 - dense_2_acc_12: 0.7474 - dense_2_acc_13: 0.8498 - dense_2_acc_14: 0.9093\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 321s 32ms/step - loss: 65.9098 - dense_2_loss: 1.0293 - dense_2_acc: 0.7126 - dense_2_acc_1: 0.0014 - dense_2_acc_2: 0.0249 - dense_2_acc_3: 0.1222 - dense_2_acc_4: 0.0111 - dense_2_acc_5: 0.0000e+00 - dense_2_acc_6: 0.0000e+00 - dense_2_acc_7: 0.0000e+00 - dense_2_acc_8: 0.0091 - dense_2_acc_9: 0.1640 - dense_2_acc_10: 0.3722 - dense_2_acc_11: 0.5872 - dense_2_acc_12: 0.7474 - dense_2_acc_13: 0.8498 - dense_2_acc_14: 0.9093\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 316s 32ms/step - loss: 65.2616 - dense_2_loss: 0.9534 - dense_2_acc: 0.7126 - dense_2_acc_1: 0.0014 - dense_2_acc_2: 0.0249 - dense_2_acc_3: 0.1337 - dense_2_acc_4: 0.0635 - dense_2_acc_5: 0.0062 - dense_2_acc_6: 7.0000e-04 - dense_2_acc_7: 1.0000e-04 - dense_2_acc_8: 0.0091 - dense_2_acc_9: 0.1640 - dense_2_acc_10: 0.3722 - dense_2_acc_11: 0.5872 - dense_2_acc_12: 0.7474 - dense_2_acc_13: 0.8498 - dense_2_acc_14: 0.9093\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 315s 32ms/step - loss: 64.7824 - dense_2_loss: 0.9002 - dense_2_acc: 0.7126 - dense_2_acc_1: 0.0014 - dense_2_acc_2: 0.0249 - dense_2_acc_3: 0.1337 - dense_2_acc_4: 0.1201 - dense_2_acc_5: 0.0212 - dense_2_acc_6: 0.0061 - dense_2_acc_7: 1.0000e-03 - dense_2_acc_8: 0.0093 - dense_2_acc_9: 0.1640 - dense_2_acc_10: 0.3722 - dense_2_acc_11: 0.5872 - dense_2_acc_12: 0.7474 - dense_2_acc_13: 0.8498 - dense_2_acc_14: 0.9093\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 325s 32ms/step - loss: 64.3865 - dense_2_loss: 0.8432 - dense_2_acc: 0.7126 - dense_2_acc_1: 0.0014 - dense_2_acc_2: 0.0249 - dense_2_acc_3: 0.1337 - dense_2_acc_4: 0.1334 - dense_2_acc_5: 0.0431 - dense_2_acc_6: 0.0159 - dense_2_acc_7: 0.0025 - dense_2_acc_8: 0.0091 - dense_2_acc_9: 0.1640 - dense_2_acc_10: 0.3722 - dense_2_acc_11: 0.5872 - dense_2_acc_12: 0.7474 - dense_2_acc_13: 0.8498 - dense_2_acc_14: 0.9093\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 320s 32ms/step - loss: 64.1045 - dense_2_loss: 0.8125 - dense_2_acc: 0.7126 - dense_2_acc_1: 0.0014 - dense_2_acc_2: 0.0249 - dense_2_acc_3: 0.1337 - dense_2_acc_4: 0.1356 - dense_2_acc_5: 0.0593 - dense_2_acc_6: 0.0276 - dense_2_acc_7: 0.0089 - dense_2_acc_8: 0.0103 - dense_2_acc_9: 0.1640 - dense_2_acc_10: 0.3722 - dense_2_acc_11: 0.5872 - dense_2_acc_12: 0.7474 - dense_2_acc_13: 0.8498 - dense_2_acc_14: 0.9093\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 319s 32ms/step - loss: 63.8259 - dense_2_loss: 0.7764 - dense_2_acc: 0.7126 - dense_2_acc_1: 0.0014 - dense_2_acc_2: 0.0249 - dense_2_acc_3: 0.1337 - dense_2_acc_4: 0.1356 - dense_2_acc_5: 0.0723 - dense_2_acc_6: 0.0397 - dense_2_acc_7: 0.0145 - dense_2_acc_8: 0.0124 - dense_2_acc_9: 0.1640 - dense_2_acc_10: 0.3722 - dense_2_acc_11: 0.5872 - dense_2_acc_12: 0.7474 - dense_2_acc_13: 0.8498 - dense_2_acc_14: 0.9093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feb13745f60>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit(\n",
    "    x=x_data,\n",
    "    y=list(decoder_output_data.swapaxes(0, 1)),\n",
    "    batch_size=100,  # correct the batch_size on big dataset\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    "#     validation_data=(x_data_val, y_data_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path, max_tokens=15):\n",
    "    \"\"\"\n",
    "    Generate a caption for the image in the given path.\n",
    "    The caption is limited to the given number of tokens (words).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and resize the image.\n",
    "    img = load_image(image_path, size=x_train.shape[1:3], color=True)\n",
    "    \n",
    "    # Expand the 3-dim numpy array to 4-dim\n",
    "    # because the image-model expects a whole batch as input,\n",
    "    # so we give it a batch with just one image.\n",
    "    image_batch = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # Process the image with the pre-trained image-model\n",
    "    encoder_output = model_encoder.predict(image_batch)\n",
    "\n",
    "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
    "    # This holds just a single sequence of integer-tokens,\n",
    "    # but the decoder-model expects a batch of sequences.\n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int64)\n",
    "\n",
    "    # The first input-token is the special start-token for 'ssss '.\n",
    "    token_int = word_to_index['<sos>']\n",
    "    \n",
    "    token_end = word_to_index['<eos>']\n",
    "\n",
    "    # Initialize an empty output-text.\n",
    "    output_text = ''\n",
    "\n",
    "    # Initialize the number of tokens we have processed.\n",
    "    count_tokens = 0\n",
    "\n",
    "    # While we haven't sampled the special end-token for '<eos>'\n",
    "    # and we haven't processed the max number of tokens.\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        # Update the input-sequence to the decoder\n",
    "        # with the last token that was sampled.\n",
    "        # In the first iteration this will set the\n",
    "        # first element to the start-token.\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "\n",
    "        # Wrap the input-data in a dict for clarity and safety,\n",
    "        # so we are sure we input the data in the right order.\n",
    "        x_data = {\n",
    "            'decoder_input': decoder_input_data,\n",
    "            'decoder_initial_hidden_state': encoder_output,\n",
    "            'decoder_initial_cell_state': np.zeros((1, state_size))\n",
    "        }\n",
    "\n",
    "        # Note that we input the entire sequence of tokens\n",
    "        # to the decoder. This wastes a lot of computation\n",
    "        # because we are only interested in the last input\n",
    "        # and output. We could modify the code to return\n",
    "        # the LSTM-states when calling predict() and then\n",
    "        # feeding these LSTM-states as well the next time\n",
    "        # we call predict(), but it would make the code\n",
    "        # much more complicated.\n",
    "        \n",
    "        # Input this data to the decoder and get the predicted output.\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "\n",
    "        # Get the last predicted token as a one-hot encoded array.\n",
    "        # Note that this is not limited by softmax, but we just\n",
    "        # need the index of the largest element so it doesn't matter.\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "\n",
    "        # Convert to an integer-token.\n",
    "        token_int = np.argmax(token_onehot)\n",
    "\n",
    "        # Lookup the word corresponding to this integer-token.\n",
    "        sampled_word = index_to_word[token_int]\n",
    "\n",
    "        # Append the word to the output-text.\n",
    "        output_text += \" \" + sampled_word\n",
    "\n",
    "        # Increment the token-counter.\n",
    "        count_tokens += 1\n",
    "\n",
    "    # This is the sequence of tokens output by the decoder.\n",
    "    output_tokens = decoder_input_data[0]\n",
    "\n",
    "    # Plot the image.\n",
    "    plt.imshow(img)  # display in RGB format\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the predicted caption.\n",
    "    print(\"Predicted caption:\")\n",
    "    print(output_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-53e052ac19e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/val2017/000000581781.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-fada14b0404f>\u001b[0m in \u001b[0;36mgenerate_caption\u001b[0;34m(image_path, max_tokens)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Note that this is not limited by softmax, but we just\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# need the index of the largest element so it doesn't matter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mtoken_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Convert to an integer-token.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "generate_caption(\"dataset/val2017/000000581781.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image(\"dataset/val2017/000000581781.jpg\", size=x_train.shape[1:3], color=True)\n",
    "image_batch = np.expand_dims(img, axis=0)\n",
    "\n",
    "# Process the image with the pre-trained image-model\n",
    "encoder_output = model_encoder.predict(image_batch)\n",
    "\n",
    "# Pre-allocate the 2-dim array used as input to the decoder.\n",
    "# This holds just a single sequence of integer-tokens,\n",
    "# but the decoder-model expects a batch of sequences.\n",
    "shape = (1, 15)\n",
    "decoder_input_data = np.zeros(shape=shape, dtype=np.int64)\n",
    "\n",
    "# The first input-token is the special start-token for 'ssss '.\n",
    "token_int = word_to_index['<sos>']\n",
    "\n",
    "token_end = word_to_index['<eos>']\n",
    "\n",
    "# Initialize an empty output-text.\n",
    "output_text = ''\n",
    "\n",
    "# Initialize the number of tokens we have processed.\n",
    "count_tokens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data[0, count_tokens] = token_int\n",
    "\n",
    "x_data = {\n",
    "    'decoder_input': decoder_input_data,\n",
    "    'decoder_initial_hidden_state': encoder_output,\n",
    "    'decoder_initial_cell_state': np.zeros((1, state_size))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[4.0185787e-07, 1.3430940e-02, 3.7616823e-07, ..., 3.4927140e-07,\n",
       "         4.5879605e-07, 4.2597284e-07]], dtype=float32),\n",
       " array([[3.5289400e-08, 2.5510447e-02, 3.1255080e-08, ..., 2.9045244e-08,\n",
       "         4.1468965e-08, 3.6445911e-08]], dtype=float32),\n",
       " array([[2.5647431e-08, 2.5205290e-02, 2.2577124e-08, ..., 2.0902322e-08,\n",
       "         3.0543379e-08, 2.6522381e-08]], dtype=float32),\n",
       " array([[2.5418602e-08, 2.4121746e-02, 2.2327749e-08, ..., 2.0667779e-08,\n",
       "         3.0450384e-08, 2.6287625e-08]], dtype=float32),\n",
       " array([[2.5897533e-08, 2.3294700e-02, 2.2724155e-08, ..., 2.1059645e-08,\n",
       "         3.1103962e-08, 2.6776135e-08]], dtype=float32),\n",
       " array([[2.6320542e-08, 2.2642400e-02, 2.3078334e-08, ..., 2.1417559e-08,\n",
       "         3.1657930e-08, 2.7205191e-08]], dtype=float32),\n",
       " array([[2.6660969e-08, 2.2052346e-02, 2.3362496e-08, ..., 2.1712186e-08,\n",
       "         3.2101386e-08, 2.7548548e-08]], dtype=float32),\n",
       " array([[2.6950804e-08, 2.1440541e-02, 2.3602311e-08, ..., 2.1969351e-08,\n",
       "         3.2480425e-08, 2.7838658e-08]], dtype=float32),\n",
       " array([[2.7207609e-08, 2.0720206e-02, 2.3811287e-08, ..., 2.2205358e-08,\n",
       "         3.2820644e-08, 2.8092590e-08]], dtype=float32),\n",
       " array([[2.7410223e-08, 1.9755466e-02, 2.3968575e-08, ..., 2.2407786e-08,\n",
       "         3.3100772e-08, 2.8286738e-08]], dtype=float32),\n",
       " array([[2.7434155e-08, 1.8290451e-02, 2.3961194e-08, ..., 2.2483457e-08,\n",
       "         3.3176025e-08, 2.8289140e-08]], dtype=float32),\n",
       " array([[2.6800544e-08, 1.5850645e-02, 2.3365553e-08, ..., 2.2052255e-08,\n",
       "         3.2475359e-08, 2.7601438e-08]], dtype=float32),\n",
       " array([[2.4380677e-08, 1.2099756e-02, 2.1199599e-08, ..., 2.0176547e-08,\n",
       "         2.9633599e-08, 2.5064230e-08]], dtype=float32),\n",
       " array([[2.0421291e-08, 8.4792608e-03, 1.7708212e-08, ..., 1.6977669e-08,\n",
       "         2.4922665e-08, 2.0960664e-08]], dtype=float32),\n",
       " array([[1.7686361e-08, 6.6298065e-03, 1.5311466e-08, ..., 1.4715515e-08,\n",
       "         2.1666722e-08, 1.8143668e-08]], dtype=float32)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output = model_decoder.predict(x_data)\n",
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9572)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3(ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
